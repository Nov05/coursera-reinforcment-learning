By the end of this module, you should be able to meet the following learning objectives:

Lesson 1: The K-Armed Bandit Problem

Understand the temporal nature of the bandit problem
Define k-armed bandit
Define action-values
Define reward

Lesson 2: What to Learn? Estimating Action Values

Define action-value estimation methods
Define exploration and exploitation
Select actions greedily using an action-value function
Define online learning
Understand a simple online sample-average action-value estimation method
Define the general online update equation
Understand why we might use a constant stepsize in the case of non-stationarity

Lesson 3: Exploration vs. Exploitation Tradeoff

Compare the short-term benefits of exploitation and the long-term benefits of exploration
Understand optimistic initial values
Describe the benefits of optimistic initial values for early exploration
Explain the criticisms of optimistic initial values
Describe the upper confidence bound action selection method
Define optimism in the face of uncertainty